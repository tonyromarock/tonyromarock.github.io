Title: Publications
Date: 2020-11-13 00:35
Category: Author
Tags: Data Science, Computer Vision
Slug: publications
Authors: Peter Mortimer
Summary: An list of my research publications.

<div class="row">
<div class="col-7 col-sm-7">
<h1 style="text-align:center">Peter Mortimer</h1>
<p><br/><br/></p>
<p style="margin: 0px;">I'm a PhD candidate at the Bundeswehr University in Munich. I'm interested in Machine Learning, Computer Vision, and Data Visualization.</p>
<p><br/></p>
<p style="margin: 0px; text-align: center;"><a href="mailto:peter.mortimer@unibw.de">Email</a> &middot; <a href="https://github.com/tonyromarock">GitHub</a> &middot; <a href="https://scholar.google.com/citations?user=ckEIQp0AAAAJ">Google Scholar</a> &middot; <a href="https://arxiv.org/search/cs?query=Mortimer%2C+Peter&searchtype=author&abstracts=show&order=-announced_date_first">arXiv</a></p>
</div>
<div class="col-5 col-sm-5">
<a href="/images/about/peter_kyoto.jpg"><img style="width:100%;max-width:100%;border-radius: 50%;" src="/images/about/peter_kyoto.jpg"/></a>
</div>
</div>

# Research

Here are all my publcations listed with links to the papers and repositories.

<hr>

<div class="row">
<div class="col-2 col-sm-2">
<a href="/images/publications/icra2024/goose-title-card.jpg" 
data-lightbox="uoa-lightbox" 
data-title="We present the German Outdoor and Offroad Dataset (GOOSE), a comprehensive dataset specifically designed for unstructured outdoor environments. The GOOSE dataset incorporates 10 000 labeled pairs of images and point clouds, which are utilized to train a range of state-of-the-art segmentation models on both image and point cloud data. We open source the dataset, along with an ontology for unstructured terrain, as well as dataset standards and guidelines." 
data-alt=""><img src="/images/publications/icra2024/goose-title-card.jpg"/></a>
</div>
<div class="col-10 col-sm-10">
<p style="margin: 0px;"><b>Peter Mortimer</b>, Raphael Hagmanns, Miguel Granero, Thorsten Luettel, Janko Petereit and Hans-Joachim Wuensche</p> 
<p style="margin: 0px;">"The GOOSE Dataset for Perception in Unstructured Environments"</p>
<p style="margin: 0px;"><i>International Conference on Robotics and Automation (<b>ICRA</b>)</i>, Yokohama, Japan, May 2024.</p>
<p><a href="https://goose-dataset.de/">Project Page</a> &middot; <a href="https://goose-dataset.de/">Paper</a> &middot; <a href="bibtex/icra2024.bib">Bibtex</a>
</div>
</div>

<hr>

<div class="row">
<div class="col-2 col-sm-2">
<a href="/images/publications/iclr2023/Dpt_MonoDepth_Position_and_Scale.png" 
data-lightbox="uoa-lightbox" 
data-title="In this blog post, we investigate the performance of the vision transformer DPT for monocular depth estimation from single images. Here we compare an object's detected depth for DPT and the fully-convolutional MonoDepth when only changing the apparent size or the vertical position of an object." 
data-alt=""><img src="/images/publications/iclr2023/Dpt_MonoDepth_Position_and_Scale.png" /></a>
</div>
<div class="col-10 col-sm-10">
<p style="margin: 0px;"><b>Peter Mortimer</b> and Hans-Joachim Wuensche</p> 
<p style="margin: 0px;">"How Do Vision Transformers See Depth in Single Images?"</p>
<p style="margin: 0px;"><i>Workshop on Scene Representations for Autonomous Driving (<b>SR4AD @ ICLR</b>)</i>, Kigali, Ruwanda, May 2023.</p>
<p><a href="https://sr4ad-vit-mde.github.io/blog/2023/visual-cues-monocular-depth-estimation/">Blog Post</a> &middot; <a href="https://sr4ad-vit-mde.github.io/web-slides/sr4ad/presentation.html">Presentation Slides</a> &middot; <a href="https://youtu.be/0ccu72jOh_k?si=jqaIerl58flkffah&t=2540">Videos</a> &middot; <a href="bibtex/sr4ad-iclr2023.bib">Bibtex</a>
</div>
</div>

<hr>

<div class="row">
<div class="col-2 col-sm-2">
<a href="/images/publications/iros2022/title-card.png" 
data-lightbox="uoa-lightbox" 
data-title="TAS-NIR is a novel dataset consisting of 209 semantically segmented and aligned VIS+NIR images in different driving scenarios in unstructured outdoor environments. The fine-grained semantic segmentation of the different vegetation and ground surface types allows closer analysis of VIS+NIR based features. The visible light color image (VIS) and near infrared image (NIR) can be combined to generate vegetation indices like the NDVI image (bottom)." 
data-alt=""><img src="/images/publications/iros2022/title-card.png" /></a>
</div>
<div class="col-10 col-sm-10">
<p style="margin: 0px;"><b>Peter Mortimer</b> and Hans-Joachim Wuensche</p> 
<p style="margin: 0px;">"TAS-NIR: A VIS+NIR Dataset for Fine-grained Semantic Segmentation in Unstructured Outdoor Environments"</p>
<p style="margin: 0px;"><i>Workshop on Planning, Perception and Navigation for Intelligent Vehicles (<b>PPNIV @ IROS</b>)</i>, Kyoto, Japan, October 2022.</p>
<p><a href="https://mucar3.de/iros2022-ppniv-tas-nir">Project Page</a> &middot; <a href="https://project.inria.fr/ppniv22/files/2022/10/PPNIV_TAS-NIR_Paper.pdf">Paper</a> &middot; <a href="https://drive.google.com/uc?export=download&id=1nSyiQDfSvWPMjx4-6Mz01qSda_kPYpsG">Dataset</a> &middot; <a href="bibtex/ppniv-iros2022.bib">Bibtex</a>
</div>
</div>

<hr>

<div class="row">
<div class="col-2 col-sm-2">
<a href="/images/publications/icpr2020/image_cube.png" 
data-lightbox="uoa-lightbox" 
data-title="TAS500 is a novel semantic segmentation dataset for autonomous driving in unstructured environments. TAS500 offers fine-grained vegetation and terrain classes to learn drivable surfaces and natural obstacles in outdoor scenes effectively." 
data-alt=""><img src="/images/publications/icpr2020/image_cube.png" /></a>
</div>
<div class="col-10 col-sm-10">
<p style="margin: 0px;">Kai Andreas Metzger, <b>Peter Mortimer</b> and Hans-Joachim Wuensche</p> 
<p style="margin: 0px;">"A Fine-Grained Dataset and its Efficient Semantic Segmentation for Unstructured Driving Scenarios"</p>
<p style="margin: 0px;"><i>International Conference on Pattern Recognition (<b>ICPR</b>)</i>, Milan, Italy, January 2021.</p>
<p><a href="https://mucar3.de/icpr2020-tas500/">Project Page</a> &middot; <a href="https://drive.google.com/file/d/1TeJK-3EBkXzD9FCq4LOLGFRD3lmKqVdp/view">Paper</a> &middot; <a href="https://rzunibw-my.sharepoint.com/:f:/g/personal/thorsten_luettel_rzunibw_onmicrosoft_com/Evdia5SSRaRPuSO2CNsNHY8B3xdK8eHpG-9DSaeEbIJyUw?e=M6NMqI">Dataset</a> &middot; <a href="bibtex/icpr2020.bib">Bibtex</a>  <!--&middot; <a href="/">GitHub</a> --> 
</div>
</div>

<hr>

# Talks

Here are all my talks that are publicly available.

<hr>

<div class="row">
<div class="col-10 col-sm-10">
<p style="margin: 0px;">"20/20 Robot Vision - How to setup cameras in ROS 1 & ROS 2 using camera_aravis"</p>
<p style="margin: 0px;">ROSCon 2022, Kyoto, Japan.</p>
<p><a href="https://vimeo.com/showcase/9954564/video/767140329">Video</a> &middot; <a href="/images/publications/roscon2022/ROSCon2022_PeterMortimer_20-20_Robot_Vision.pdf">Slides</a> &middot; <a href="https://github.com/FraunhoferIOSB/camera_aravis">GitHub</a> 
</div>
<div class="col-2 col-sm-2">
<a href="/images/publications/roscon2022/title-card.png" 
data-lightbox="uoa-lightbox" 
data-title="Industrial camera drivers like aravis give you control over many components of the image acquisiton process. Here you can observe the difference between minizing the gain (top) and minimizing the exposure time (bottom) on the amount of noise in the image." 
data-alt=""><img src="/images/publications/roscon2022/title-card.png"/></a>
</div>
</div>